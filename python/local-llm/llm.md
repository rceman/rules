# ðŸ§  Local LLM Rules

## 1. Model Choice

| Useâ€‘case         | Recommended model          | Size (Q4_K_M) |
|------------------|----------------------------|---------------|
| General planning | **Qwen3â€‘1.7B**             | 2â€¯GB          |
| Code generation  | **Qwen3â€‘8B**               | 9â€¯GB          |

Quantisation: prefer **Q4_K_M** or **Q6_K** for balance of speed & quality.

## 2. Storage

Store weights under `models/` (gitâ€‘ignored).  
Provide SHAâ€‘256 checksums in `scripts/download_models.sh`.

## 3. Loading

```python
from llama_cpp import Llama

llm = Llama(
    model_path="models/qwen3-8b-q4_k_m.gguf",
    n_gpu_layers=-1,           # fullâ€‘GPU
    context_length=32768
)
```

## 4. Prompt Engineering

* Keep templates in files under `prompts/`.
* Never build prompts by naÃ¯ve string concatenation; use `.format()` or **jinja2**.

## 5. Safety

* Strip PII from prompts before logging.
* Add a contentâ€‘filter LLM or ruleâ€‘based guard for user input.
